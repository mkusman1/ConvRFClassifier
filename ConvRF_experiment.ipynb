{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvRF_experiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNGY++Qkq/u7oAJQlVTfoiF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkusman1/ConvRFClassifier/blob/master/ConvRF_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "CmfayjlnfST4",
        "outputId": "4c37b36e-7b08-436e-851e-c14bbe54dadc"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-39cb94a8-871b-4f22-b098-be9d6827e36b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-39cb94a8-871b-4f22-b098-be9d6827e36b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ConvRFClassifier.py to ConvRFClassifier (4).py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ConvRFClassifier.py': b'import numpy as np\\r\\nfrom sklearn.base import BaseEstimator\\r\\nfrom sklearn.ensemble import RandomForestClassifier\\r\\n\\r\\n#problem with pickling\\r\\nclass ConvRFClassifier(BaseEstimator):\\r\\n    \\r\\n   \\r\\n    def __init__(self, layers = 1, kernel_size = (5,), stride = (2,), n_estimators = 100, num_outputs = 10):\\r\\n        \"\"\"A convolutional random forest classifier.\\r\\n        Initializes a ConvRFClassifier.\\r\\n        Parameters\\r\\n        ----------\\r\\n        layers : int, default = 1\\r\\n            How many layers of convolutions.\\r\\n        kernel_size: tuple of integers, default = (5,)\\r\\n            Size of each kernel at each layer. Length should be equal to layers.\\r\\n        stride: tuple of integers, default = (5,)\\r\\n            Number of pixels skipped per image segment at each layer. \\r\\n            Length should be equal to layers.\\r\\n        n_estimators: integer, default = 100\\r\\n            Number of trees in final random forest classifier.\\r\\n        num_outputs: integer, default = 10\\r\\n            Number of trees in the forest that convolve image segments.\\r\\n        Returns\\r\\n        -------\\r\\n        \"\"\"\\r\\n        \\r\\n        if not (len(kernel_size) == layers and len(stride) == layers):\\r\\n            raise Exception(\\'Length of kernel_sizes and strides must be same as layers\\')\\r\\n        self.kernel_size = kernel_size\\r\\n        self.stride = stride\\r\\n        self.num_outputs = num_outputs\\r\\n        self.n_estimators = n_estimators\\r\\n        self.layers = layers\\r\\n        #initialize Random Forest array to hold kernels\\r\\n        self.kernel_forests = [None] * self.layers\\r\\n\\r\\n\\r\\n    def segment(self, images, kernel_size, stride, labels=None, flatten=True):\\r\\n        \"\"\"Segment the images into multiple chunks.\\r\\n        Parameters\\r\\n        ----------\\r\\n        images : array-like, of shape (n_samples, dimension_1, dimension_2, channel)\\r\\n            Input data. dimension_1 should be equal to dimension_2.\\r\\n        kernel_size: integer\\r\\n            Size of each image segment\\r\\n        stride: integer\\r\\n            Number of pixels skipped per image segment.\\r\\n        labels: array-like, of shape (n_samples, dimension_1, dimension_2, 1), default=None\\r\\n            True class labels for each image segment.\\r\\n        flatten: boolean, default=True\\r\\n            Whether image segments should be flattened. Default is true.\\r\\n        Returns\\r\\n        -------\\r\\n        out_images : array-like, of shape (n_samples, dimension_1, dimension_2, dimension_3)\\r\\n            Image segments. dimension_1 = dimension_2 and dimension_3 = kernel_size * \\r\\n            kernel_size * channels if flatten is true.\\r\\n        out_labels : array-like, of shape (n_samples, dimension_1, dimension_2)\\r\\n            True image labels for each image segment. Same for all segments taken from\\r\\n            the same image.\\r\\n        out_dim : integer\\r\\n            Dimension of output equal to dimension_1 and dimension_2. Calculated by\\r\\n            (dimension of input - kernel_size) / stride + 1.\\r\\n        \"\"\"\\r\\n        \\r\\n        if images.shape[1] != images.shape[2]:\\r\\n            raise Exception(\\'Only square images are allowed\\')\\r\\n            \\r\\n        #get dimensions of input and output\\r\\n        batch_size, in_dim, _, num_channels = images.shape\\r\\n        out_dim = int((in_dim - kernel_size) / stride) + 1  # calculate output dimensions\\r\\n\\r\\n        # create matrix to hold the chopped imagesO\\r\\n        out_images = np.zeros((batch_size, out_dim, out_dim,\\r\\n                               kernel_size, kernel_size, num_channels))\\r\\n\\r\\n        curr_y = out_y = 0\\r\\n        # move kernel vertically across the image\\r\\n        while curr_y + kernel_size <= in_dim:\\r\\n            curr_x = out_x = 0\\r\\n            # move kernel horizontally across the image\\r\\n            while curr_x + kernel_size <= in_dim:\\r\\n                # chop images\\r\\n                out_images[:, out_x, out_y] = images[:, curr_x:curr_x +\\r\\n                                                     kernel_size, curr_y:curr_y + kernel_size, :]\\r\\n                #increment x and x index\\r\\n                curr_x += stride\\r\\n                out_x += 1\\r\\n            #increment y and y index\\r\\n            curr_y += stride\\r\\n            out_y += 1\\r\\n\\r\\n        #flatten image into vector by row major order\\r\\n        if flatten:\\r\\n            out_images = out_images.reshape(batch_size, out_dim, out_dim, -1)\\r\\n            \\r\\n        #flatten labels if any\\r\\n        out_labels = None\\r\\n        if labels is not None:\\r\\n            out_labels = np.zeros((batch_size, out_dim, out_dim))\\r\\n            out_labels[:, ] = labels.reshape(-1, 1, 1)\\r\\n\\r\\n        return out_images, out_labels, out_dim\\r\\n\\r\\n\\r\\n    def convolve_fit(self, images, labels):\\r\\n        \"\"\"Fit the random forests for convolution.\\r\\n        Parameters\\r\\n        ----------\\r\\n        images : array-like, of shape (n_samples, dimension_1, dimension_2, dimension_3)\\r\\n            Input data. dimension_1 should be equal to dimension_2. dimension_3 should\\r\\n            be flattened vector of image segments.\\r\\n        labels: array-like, of shape (n_samples, dimension_1, dimension_2)\\r\\n            True class labels for each image segment.\\r\\n        Returns\\r\\n        -------\\r\\n        images : array-like, of shape (n_samples, dimension_1, dimension_2, num_outputs)\\r\\n            convoluted results for each image segment for each tree.\\r\\n        \"\"\"\\r\\n\\r\\n        #initialize Random Forest Classifier that does the final classification\\r\\n        self.random_forest = RandomForestClassifier(n_estimators = self.n_estimators, n_jobs = -1)\\r\\n        #convolve self.layers times\\r\\n        for layer in range(self.layers):\\r\\n            #get input image segments\\r\\n            sub_images, sub_labels, out_dim = self.segment(images, self.kernel_size[layer], self.stride[layer], \\\\\\r\\n                                                             labels=labels, flatten=True)\\r\\n            \\r\\n            #initiate another array to hold kernels for each image segment\\r\\n            self.kernel_forests[layer] = [[0]*out_dim for _ in range(out_dim)]\\r\\n            convolved_image = np.zeros((images.shape[0], out_dim, out_dim, self.num_outputs))\\r\\n            \\r\\n            #iterate through length and width of convolved image segments\\r\\n            for i in range(out_dim):\\r\\n                for j in range(out_dim):\\r\\n                    #initialize Random Forests to act like kernels\\r\\n                    self.kernel_forests[layer][i][j] = RandomForestClassifier(n_estimators=self.num_outputs, max_depth=6, n_jobs = -1)\\r\\n                    #fit the kernels\\r\\n                    self.kernel_forests[layer][i][j].fit(sub_images[:, i, j], sub_labels[:, i, j])\\r\\n                    #convolution step, image segment -> int\\r\\n                    convolved_image[:, i, j] = self.kernel_forests[layer][i][j].apply(sub_images[:, i, j])\\r\\n            images = convolved_image\\r\\n        return images\\r\\n\\r\\n\\r\\n    def convolve_predict(self, images):\\r\\n        \"\"\"Convolve images using random forests.\\r\\n        Parameters\\r\\n        ----------\\r\\n        images : array-like, of shape (n_samples, dimension_1, dimension_2, dimension_3)\\r\\n            Input data. dimension_1 should be equal to dimension_2. dimension_3 should\\r\\n            be flattened vector of image segments.\\r\\n        Returns\\r\\n        -------\\r\\n        images : array-like, of shape (n_samples, dimension_1, dimension_2, num_outputs)\\r\\n            convoluted results for each image segment for each tree.\\r\\n        \"\"\"\\r\\n        #check if forest is fit\\r\\n        if not self.kernel_forests[0]:\\r\\n            raise Exception(\"Should fit training data before predicting\")\\r\\n        \\r\\n        #repeat for each layer\\r\\n        for layer in range(self.layers):\\r\\n            #segment image\\r\\n            sub_images, _, out_dim = self.segment(images, self.kernel_size[layer], self.stride[layer], flatten=True)\\r\\n            \\r\\n            #initialize convolved images\\r\\n            kernel_predictions = np.zeros((images.shape[0], out_dim, out_dim, self.num_outputs))\\r\\n            for i in range(out_dim):\\r\\n                for j in range(out_dim):\\r\\n                    #convolved_image[:, i, j] = self.kernel_forests[i][j].predict_proba(sub_images[:, i, j])\\r\\n                    #apply convolution\\r\\n                    kernel_predictions[:, i, j] = self.kernel_forests[layer][i][j].apply(sub_images[:, i, j])\\r\\n            images = kernel_predictions           \\r\\n        return images  \\r\\n \\r\\n    \\r\\n    def fit(self, images, labels):\\r\\n        \"\"\"Fit estimator.\\r\\n        Parameters\\r\\n        ----------\\r\\n        X : array-like, of shape (n_samples, dimension_1, dimension_2, channels)\\r\\n            Input data.  Array of 3 dimensional images.\\r\\n        y : array-like, 1D numpy array\\r\\n            Labels\\r\\n        Returns\\r\\n        -------\\r\\n        self : object\\r\\n        \"\"\"\\r\\n        #fit the kernels and convolve images\\r\\n        im = self.convolve_fit(images, labels)\\r\\n        im = im.reshape(len(images), -1)\\r\\n        #fit the classifier\\r\\n        self.random_forest.fit(im, labels)\\r\\n\\r\\n       \\r\\n    def predict(self, images):\\r\\n        \"\"\"Predict class for X.\\r\\n        Parameters\\r\\n        ----------\\r\\n        X : array-like, of shape (n_samples, dimension_1, dimension_2, channels)\\r\\n            Input data.  Array of 3 dimensional images.\\r\\n        Returns\\r\\n        -------\\r\\n        y : array-like of shape (n_samples,)\\r\\n            Returns predicted class labels for samples X.\\r\\n        \"\"\"\\r\\n        #convolve the images\\r\\n        im = self.convolve_predict(images)\\r\\n        im = im.reshape(len(images), -1)\\r\\n        #predict the labels\\r\\n        return self.random_forest.predict(im)\\r\\n    \\r\\n    \\r\\n    def predict_proba(self, images):\\r\\n        \"\"\"Predict class probabilities for X.\\r\\n        The predicted class probabilities of an input sample are computed as\\r\\n        the mean predicted class of the trees in the forest.\\r\\n        Parameters\\r\\n        ----------\\r\\n        X : array_like of shape (n_samples, dimension_1, dimension_2, channels)\\r\\n            Input data.  Array of 3 dimensional images.\\r\\n        Returns\\r\\n        -------\\r\\n        p : array-like of shape (n_samples,)\\r\\n            The class probabilities of the input samples. The order of the\\r\\n            classes corresponds to that in the attribute `classes_`.\\r\\n        \"\"\"\\r\\n        #convolve the images\\r\\n        im = self.convolve_predict(self, images)\\r\\n        im = im.reshape(len(images), -1)\\r\\n        #predict the probability of labels\\r\\n        return self.random_forest.predict_proba(im)'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr3RFC0JhF-f"
      },
      "source": [
        "# general imports\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import ConvRFClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "import seaborn as sns; sns.set()\n",
        "plt.rcParams[\"legend.loc\"] = \"best\"\n",
        "plt.rcParams['figure.facecolor'] = 'white'\n",
        "#%matplotlib inline\n",
        "\n",
        "names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSj5GEachLei",
        "outputId": "779f9e28-b063-4953-8c74-e037a5ad282d"
      },
      "source": [
        "# filter python warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def run():\n",
        "    torch.multiprocessing.freeze_support()\n",
        "    print('loop')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO7zWsiGhSHE",
        "outputId": "96604fde-3fa6-470b-bbdf-246a43d79d9c"
      },
      "source": [
        "# prepare CIFAR data\n",
        "\n",
        "# normalize\n",
        "scale = np.mean(np.arange(0, 256))\n",
        "normalize = lambda x: (x - scale) / scale\n",
        "\n",
        "# train data\n",
        "cifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
        "cifar_train_images = normalize(cifar_trainset.data)\n",
        "cifar_train_labels = np.array(cifar_trainset.targets)\n",
        "\n",
        "# test data\n",
        "cifar_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
        "cifar_test_images = normalize(cifar_testset.data)\n",
        "cifar_test_labels = np.array(cifar_testset.targets)\n",
        "\n",
        "\n",
        "# transform\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "testset = datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_dFX0wWhoOt"
      },
      "source": [
        "# define a simple CNN arhcitecture\n",
        "class SimpleCNNOneFilter(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(SimpleCNNOneFilter, self).__init__()        \n",
        "        self.conv1 = torch.nn.Conv2d(3, 1, kernel_size=10, stride=2)\n",
        "        self.fc1 = torch.nn.Linear(144, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = x.view(-1, 144)\n",
        "        x = self.fc1(x)\n",
        "        return(x)\n",
        "\n",
        "class SimpleCNN32Filter(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(SimpleCNN32Filter, self).__init__()        \n",
        "        self.conv1 = torch.nn.Conv2d(3, 32, kernel_size=10, stride=2) # try 64 too, if possible\n",
        "        self.fc1 = torch.nn.Linear(144*32, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = x.view(-1, 144*32)\n",
        "        x = self.fc1(x)\n",
        "        return(x)\n",
        "\n",
        "class SimpleCNN32Filter2Layers(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(SimpleCNN32Filter2Layers, self).__init__()        \n",
        "        self.conv1 = torch.nn.Conv2d(3, 32, kernel_size=10, stride=2)\n",
        "        self.conv2 = torch.nn.Conv2d(32, 32, kernel_size=7, stride=1)\n",
        "        self.fc1 = torch.nn.Linear(36*32, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(-1, 36*32)\n",
        "        x = self.fc1(x)\n",
        "        return(x)\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvtwkySVhssl"
      },
      "source": [
        "# changed to multiple labels\n",
        "def run_rf(model, train_images, train_labels, test_images, test_labels, fraction_of_train_samples):\n",
        "    num_train_samples_class_1 = int(np.sum(train_labels==class1) * fraction_of_train_samples)\n",
        "    num_train_samples_class_2 = int(np.sum(train_labels==class2) * fraction_of_train_samples)\n",
        "    num_train_samples_class_3 = int(np.sum(train_labels==class3) * fraction_of_train_samples)\n",
        "\n",
        "    # get only train images and labels for class 1 and class 2\n",
        "    train_images = np.concatenate([train_images[train_labels==class1][:num_train_samples_class_1], train_images[train_labels==class2][:num_train_samples_class_2],train_images[train_labels==class3][:num_train_samples_class_3]])\n",
        "    train_labels = np.concatenate([np.repeat(0, num_train_samples_class_1), np.repeat(1, num_train_samples_class_2), np.repeat(2, num_train_samples_class_3)])\n",
        "\n",
        "    # get only test images and labels for class 1 and class 2\n",
        "    test_images = np.concatenate([test_images[test_labels==class1], test_images[test_labels==class2],test_images[test_labels==class3]])\n",
        "    test_labels = np.concatenate([np.repeat(0, np.sum(test_labels==class1)), np.repeat(1, np.sum(test_labels==class2)),  np.repeat(2, np.sum(test_labels==class2))])\n",
        "\n",
        "    if isinstance(model, sklearn.ensemble.RandomForestClassifier):\n",
        "        train_images = train_images.reshape(-1, 32*32*3)\n",
        "        test_images = test_images.reshape(-1, 32*32*3)\n",
        "    model.fit(train_images, train_labels)\n",
        "    # Test\n",
        "    test_preds = model.predict(test_images)\n",
        "    return accuracy_score(test_labels, test_preds)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7YUY-Sah1aV"
      },
      "source": [
        "def cnn_train_test(cnn_model, y_train, y_test, fraction_of_train_samples, class1=3, class2=8, class3 = 9):\n",
        "    # set params\n",
        "    num_epochs = 5\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    class1_indices = np.argwhere(y_train==class1).flatten()\n",
        "    class1_indices = class1_indices[:int(len(class1_indices) * fraction_of_train_samples)]\n",
        "    class2_indices = np.argwhere(y_train==class2).flatten()\n",
        "    class2_indices = class2_indices[:int(len(class2_indices) * fraction_of_train_samples)]\n",
        "    class3_indices = np.argwhere(y_train==class3).flatten()\n",
        "    class3_indices = class3_indices[:int(len(class3_indices) * fraction_of_train_samples)]\n",
        "    train_indices = np.concatenate([class1_indices, class2_indices, class3_indices])\n",
        "\n",
        "    train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices)\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=32, num_workers=2, sampler=train_sampler)\n",
        "\n",
        "    test_indices = np.concatenate([np.argwhere(y_test==class1).flatten(), np.argwhere(y_test==class2).flatten(), np.argwhere(y_test==class3).flatten()])\n",
        "    test_sampler = torch.utils.data.sampler.SubsetRandomSampler(test_indices)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
        "                                             shuffle=False, num_workers=2, sampler=test_sampler)\n",
        "    # define model\n",
        "    net = cnn_model()\n",
        "    dev = torch.device(\"cuda:0\")\n",
        "    net.to(dev)\n",
        "    # loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            # get the inputs\n",
        "            inputs, labels = data\n",
        "            inputs = torch.tensor(inputs).to(dev)\n",
        "            labels = torch.tensor(labels).to(dev)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # test the model\n",
        "    correct = torch.tensor(0).to(dev)\n",
        "    total = torch.tensor(0).to(dev)\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            labels = torch.tensor(labels).to(dev)\n",
        "            images = torch.tensor(images).to(dev)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels.view(-1)).sum().item()\n",
        "    accuracy = float(correct) / float(total)\n",
        "    return accuracy"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFdzUOCmiAkP"
      },
      "source": [
        "def run_cnn(cnn_model, train_images, train_labels, test_images, test_labels, fraction_of_train_samples, class1=3, class2=8, class3 = 9):\n",
        "    return cnn_train_test(cnn_model, train_labels, test_labels, fraction_of_train_samples, class1, class2,class3)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_Fv4WhLf7Ez",
        "outputId": "8499b3fb-ccb6-4c98-b0e4-6d2ce8de4837"
      },
      "source": [
        "for class1 in range(10):\n",
        "    for class2 in range(class1 + 1, 10):\n",
        "       for class3 in range(class2 + 1, 10):\n",
        "        # accuracy vs num training samples (naive_rf)\n",
        "        \n",
        "          naive_rf_acc_vs_n = list()\n",
        "          fraction_of_train_samples_space = np.geomspace(0.01, 1, num=8)\n",
        "          for fraction_of_train_samples in fraction_of_train_samples_space:\n",
        "             RF = RandomForestClassifier(n_estimators=100, n_jobs = -1)\n",
        "             best_accuracy = np.mean([run_rf(RF, cifar_train_images, cifar_train_labels, cifar_test_images, cifar_test_labels, fraction_of_train_samples) for _ in range(2)])\n",
        "             naive_rf_acc_vs_n.append(best_accuracy)\n",
        "             print(\"Train Fraction:\", str(fraction_of_train_samples))\n",
        "             print(\"Accuracy:\", str(best_accuracy))\n",
        "            \n",
        "        \n",
        "           # accuracy vs num training samples (naive_rf)\n",
        "         \n",
        " #         conv_rf_2_layer = list()\n",
        " #         for fraction_of_train_samples in fraction_of_train_samples_space:\n",
        " #             conv_rf_2l = ConvRFClassifier.ConvRFClassifier(layers = 2, kernel_size = (10, 5), stride = (2, 1))\n",
        " #             best_accuracy = np.mean([run_rf(conv_rf_2l, cifar_train_images, cifar_train_labels, cifar_test_images, cifar_test_labels, fraction_of_train_samples) for _ in range(2)])\n",
        " #             conv_rf_2_layer.append(best_accuracy)\n",
        " #             print(\"Train Fraction:\", str(fraction_of_train_samples))\n",
        " #             print(\"Accuracy:\", str(best_accuracy))\n",
        "\n",
        "          \n",
        "        # accuracy vs num training samples (naive_rf)\n",
        "          conv_rf_apply = list()\n",
        "          for fraction_of_train_samples in fraction_of_train_samples_space:\n",
        "              conv_rf_a = ConvRFClassifier.ConvRFClassifier(layers = 1, kernel_size = (10,), stride = (2,))\n",
        "              best_accuracy = np.mean([run_rf(conv_rf_a, cifar_train_images, cifar_train_labels, cifar_test_images, cifar_test_labels, fraction_of_train_samples) for _ in range(2)])\n",
        "              conv_rf_apply.append(best_accuracy)\n",
        "              print(\"Train Fraction:\", str(fraction_of_train_samples))\n",
        "              print(\"Accuracy:\", str(best_accuracy))\n",
        "        \n",
        "            \n",
        "#        # accuracy vs num training samples (one layer cnn (32 filters))\n",
        "#          cnn32_acc_vs_n = list()\n",
        "#          for fraction_of_train_samples in fraction_of_train_samples_space:\n",
        "#              best_accuracy = np.mean([run_cnn(SimpleCNN32Filter, cifar_train_images, cifar_train_labels, cifar_test_images, cifar_test_labels, fraction_of_train_samples) for _ in range(2)])\n",
        "#              cnn32_acc_vs_n.append(best_accuracy)\n",
        "#              print(\"Train Fraction:\", str(fraction_of_train_samples))\n",
        "#              print(\"Accuracy:\", str(best_accuracy))\n",
        "            \n",
        "        \n",
        "        # accuracy vs num training samples (two layer cnn (32 filters))\n",
        "          cnn32_two_layer_acc_vs_n = list()\n",
        "          for fraction_of_train_samples in fraction_of_train_samples_space:\n",
        "              best_accuracy = np.mean([run_cnn(SimpleCNN32Filter2Layers, cifar_train_images, cifar_train_labels, cifar_test_images, cifar_test_labels, fraction_of_train_samples) for _ in range(3)])\n",
        "              cnn32_two_layer_acc_vs_n.append(best_accuracy)\n",
        "              print(\"Train Fraction:\", str(fraction_of_train_samples))\n",
        "              print(\"Accuracy:\", str(best_accuracy))\n",
        "            \n",
        "#        # accuracy vs num training samples (one layer cnn)\n",
        "#          cnn_acc_vs_n = list()\n",
        "#          for fraction_of_train_samples in fraction_of_train_samples_space:\n",
        "#              best_accuracy = np.mean([run_cnn(SimpleCNNOneFilter, cifar_train_images, cifar_train_labels, cifar_test_images, cifar_test_labels, fraction_of_train_samples, 0, 2) for _ in range(2)])\n",
        "#              cnn_acc_vs_n.append(best_accuracy)\n",
        "#              print(\"Train Fraction:\", str(fraction_of_train_samples))\n",
        "#              print(\"Accuracy:\", str(best_accuracy))\n",
        " \n",
        "            \n",
        "            \n",
        "          plt.rcParams['figure.figsize'] = 13, 10\n",
        "          plt.rcParams['font.size'] = 25\n",
        "          plt.rcParams['legend.fontsize'] = 16.5\n",
        "          plt.rcParams['legend.handlelength'] = 2.5\n",
        "          plt.rcParams['figure.titlesize'] = 20\n",
        "          plt.rcParams['xtick.labelsize'] = 15\n",
        "          plt.rcParams['ytick.labelsize'] = 15\n",
        "        \n",
        "          fig, ax = plt.subplots() # create a new figure with a default 111 subplot\n",
        "          ax.plot(fraction_of_train_samples_space*5000, naive_rf_acc_vs_n, marker='X', markerfacecolor='red', markersize=8, color='green', linewidth=3, linestyle=\":\", label=\"Naive RF\")\n",
        "          ax.plot(fraction_of_train_samples_space*5000, conv_rf_apply, marker='X', markerfacecolor='red', markersize=8, color='green', linewidth=3, linestyle=\"--\", label=\"Conv RF\")\n",
        " #         ax.plot(fraction_of_train_samples_space*5000, conv_rf_2_layer, marker='X', markerfacecolor='red', markersize=8, color='green', linewidth=3, label=\"Conv RF 2 layer\")\n",
        "        \n",
        "#          ax.plot(fraction_of_train_samples_space*5000, cnn_acc_vs_n, marker='X', markerfacecolor='red', markersize=8, color='orange', linewidth=3, linestyle=\":\", label=\"Simple CNN\")\n",
        "#          ax.plot(fraction_of_train_samples_space*5000, cnn32_acc_vs_n, marker='X', markerfacecolor='red', markersize=8, color='orange', linewidth=3, linestyle=\"--\", label=\"CNN (32 filters)\")\n",
        "          ax.plot(fraction_of_train_samples_space*5000, cnn32_two_layer_acc_vs_n, marker='X', markerfacecolor='red', markersize=8, color='orange', linewidth=3, label=\"CNN Two Layer (32 filters)\")\n",
        "        \n",
        "          ax.set_xlabel('Number of Train Samples', fontsize=18)\n",
        "          ax.set_xscale('log')\n",
        "          ax.set_xticks([i*5000 for i in list(np.geomspace(0.01, 1, num=8))])\n",
        "          ax.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
        "        \n",
        "          ax.set_ylabel('Accuracy', fontsize=18)\n",
        "        \n",
        "          ax.set_title(str(class1) + \" (\" + names[class1] + \") vs \" + str(class2) + \"(\" + names[class2] + \") vs \" + str(class3) + \"(\" + names[class3] + \") classification\", fontsize=18)\n",
        "          plt.legend()\n",
        "          from google.colab import files\n",
        "          plt.savefig( str(class1) + \"_vs_\" + str(class2) +  \"_vs_\" + str(class3)+\".png\")\n",
        "          files.download( str(class1) + \"_vs_\" + str(class2) + \"_vs_\" + str(class3)+\".png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Fraction: 0.01\n",
            "Accuracy: 0.6611666666666667\n",
            "Train Fraction: 0.019306977288832496\n",
            "Accuracy: 0.676\n",
            "Train Fraction: 0.0372759372031494\n",
            "Accuracy: 0.7091666666666667\n",
            "Train Fraction: 0.07196856730011521\n",
            "Accuracy: 0.7288333333333333\n",
            "Train Fraction: 0.13894954943731375\n",
            "Accuracy: 0.7531666666666668\n",
            "Train Fraction: 0.2682695795279725\n",
            "Accuracy: 0.7648333333333334\n",
            "Train Fraction: 0.517947467923121\n",
            "Accuracy: 0.778\n",
            "Train Fraction: 1.0\n",
            "Accuracy: 0.7915\n",
            "Train Fraction: 0.01\n",
            "Accuracy: 0.6536666666666666\n",
            "Train Fraction: 0.019306977288832496\n",
            "Accuracy: 0.6758333333333333\n",
            "Train Fraction: 0.0372759372031494\n",
            "Accuracy: 0.6991666666666667\n",
            "Train Fraction: 0.07196856730011521\n",
            "Accuracy: 0.7176666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNAhsM_Fi8JD"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUGFoDDlJx18"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}